{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tune Your Own Model with Hugging Face\n",
        "\n",
        "This notebook provides a comprehensive guide to fine-tuning pre-trained models using the Hugging Face ecosystem. We'll cover the entire workflow from understanding when to fine-tune, to deploying your model in production.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction and Motivation](#1-introduction-and-motivation)\n",
        "2. [Data Collection and Preparation](#2-data-collection-and-preparation)\n",
        "3. [Medical Reasoning with II-Medical-RL Dataset](#3-medical-reasoning-with-ii-medical-rl-dataset)\n",
        "4. [Selecting a Base Model](#4-selecting-a-base-model)\n",
        "5. [Training and Fine-tuning](#5-training-and-fine-tuning)\n",
        "6. [Evaluation](#6-evaluation)\n",
        "7. [Deployment](#7-deployment)\n",
        "\n",
        "Let's start by installing the necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets evaluate accelerate scikit-learn pandas matplotlib pillow torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import common libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoFeatureExtractor\n",
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "from transformers import EarlyStoppingCallback\n",
        "import torch\n",
        "import evaluate\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from PIL import Image\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction and Motivation\n",
        "\n",
        "### Why Fine-tune Rather Than Train from Scratch?\n",
        "\n",
        "Fine-tuning leverages pre-trained models that have already learned general patterns from large datasets, allowing you to adapt them to your specific task with relatively little data and computational resources.\n",
        "\n",
        "Key advantages include:\n",
        "\n",
        "- **Data Efficiency**: Requires significantly less task-specific data (often 10-100x less)\n",
        "- **Computational Efficiency**: Training takes hours instead of weeks/months\n",
        "- **Better Performance**: Especially with limited data, fine-tuned models outperform models trained from scratch\n",
        "- **Knowledge Transfer**: Leverages general knowledge learned during pre-training\n",
        "\n",
        "### Common Scenarios and Use Cases\n",
        "\n",
        "Fine-tuning is particularly valuable in these scenarios:\n",
        "\n",
        "1. **Domain Adaptation**: Adapting general models to specific domains (medical, legal, financial)\n",
        "2. **Task Specialization**: Adapting a model trained for one task to a related task\n",
        "3. **Low-Resource Settings**: When you have limited data or computational resources\n",
        "4. **Multilingual Applications**: Adapting language models to specific languages\n",
        "5. **Rapid Prototyping**: Quickly testing ideas and solutions\n",
        "\n",
        "### When to Consider Fine-tuning\n",
        "\n",
        "| Scenario | Recommendation |\n",
        "|----------|----------------|\n",
        "| Limited data (<10k examples) | **Fine-tune** - Training from scratch likely won't work well |\n",
        "| Limited compute | **Fine-tune** - Significantly lower resource requirements |\n",
        "| Tight timeline | **Fine-tune** - Faster to reach acceptable performance |\n",
        "| Very different domain from available pre-trained models | **Consider domain-adaptive pre-training** before fine-tuning |\n",
        "| Need for architectural innovation | **Train from scratch** if your architecture differs significantly |\n",
        "\n",
        "Now that we understand why fine-tuning is valuable, let's dive into the most critical component of successful fine-tuning: your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Collection and Preparation\n",
        "\n",
        "The quality, quantity, and relevance of your data is the single most important factor in fine-tuning success. Even the most sophisticated models can't overcome fundamental data problems (\"garbage in, garbage out\").\n",
        "\n",
        "### 2.1 Strategies for Data Collection\n",
        "\n",
        "Let's explore different approaches to collecting task-specific data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Using Existing Public Datasets\n",
        "\n",
        "The simplest approach is to leverage existing datasets from the Hugging Face Hub. Let's see how to browse and load datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Loading a text classification dataset for sentiment analysis\n",
        "imdb_dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Examine the dataset structure\n",
        "print(f\"Dataset structure: {imdb_dataset}\")\n",
        "print(f\"Available splits: {list(imdb_dataset.keys())}\")\n",
        "print(f\"Number of examples in train split: {len(imdb_dataset['train'])}\")\n",
        "print(f\"Features: {imdb_dataset['train'].features}\")\n",
        "\n",
        "# Look at a sample example\n",
        "print(\"\\nSample example:\")\n",
        "print(imdb_dataset['train'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Loading an image classification dataset\n",
        "cifar10_dataset = load_dataset(\"cifar10\")\n",
        "\n",
        "# Examine the dataset structure\n",
        "print(f\"Dataset structure: {cifar10_dataset}\")\n",
        "print(f\"Available splits: {list(cifar10_dataset.keys())}\")\n",
        "print(f\"Number of examples in train split: {len(cifar10_dataset['train'])}\")\n",
        "print(f\"Features: {cifar10_dataset['train'].features}\")\n",
        "\n",
        "# Visualize a few examples\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, idx in enumerate(range(10)):\n",
        "    example = cifar10_dataset['train'][idx]\n",
        "    axes[i].imshow(example['img'])\n",
        "    axes[i].set_title(f\"Label: {example['label']}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating Custom Datasets from Structured Sources\n",
        "\n",
        "Often, you'll need to create a custom dataset from existing structured sources like CSV files or JSON data. Let's see how to create a custom dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a sample CSV file with product reviews\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic product review data\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "\n",
        "# Create product categories\n",
        "categories = ['Electronics', 'Clothing', 'Home', 'Books', 'Beauty']\n",
        "\n",
        "# Generate synthetic reviews\n",
        "positive_templates = [\n",
        "    \"I love this {category} product! It's exactly what I needed.\",\n",
        "    \"This {category} item exceeded my expectations. Highly recommend!\",\n",
        "    \"Great quality {category} product, worth every penny.\",\n",
        "    \"The best {category} purchase I've made this year.\",\n",
        "    \"Excellent {category} item, works perfectly.\"\n",
        "]\n",
        "\n",
        "negative_templates = [\n",
        "    \"Disappointed with this {category} product. Would not recommend.\",\n",
        "    \"Poor quality {category} item, broke after a few uses.\",\n",
        "    \"This {category} product didn't meet my expectations at all.\",\n",
        "    \"Waste of money for this {category} item.\",\n",
        "    \"The {category} product has serious design flaws.\"\n",
        "]\n",
        "\n",
        "# Generate data\n",
        "data = []\n",
        "for i in range(n_samples):\n",
        "    category = np.random.choice(categories)\n",
        "    rating = np.random.randint(1, 6)  # 1-5 star rating\n",
        "    \n",
        "    if rating >= 4:  # Positive review\n",
        "        template = np.random.choice(positive_templates)\n",
        "        sentiment = 'positive'\n",
        "    else:  # Negative review\n",
        "        template = np.random.choice(negative_templates)\n",
        "        sentiment = 'negative'\n",
        "    \n",
        "    review = template.format(category=category.lower())\n",
        "    \n",
        "    # Add some noise/variation\n",
        "    if np.random.random() < 0.3:\n",
        "        review = review + \" \" + np.random.choice([\"Thank you!\", \"Never again.\", \"Will buy again.\", \"Save your money.\"])\n",
        "    \n",
        "    data.append({\n",
        "        'review_id': i,\n",
        "        'category': category,\n",
        "        'rating': rating,\n",
        "        'review_text': review,\n",
        "        'sentiment': sentiment\n",
        "    })\n",
        "\n",
        "# Create DataFrame and save to CSV\n",
        "df = pd.DataFrame(data)\n",
        "csv_path = 'product_reviews.csv'\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "# Display the first few rows\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the CSV to a Hugging Face dataset\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Load the CSV data\n",
        "df = pd.read_csv('product_reviews.csv')\n",
        "\n",
        "# Create train/validation/test splits (80/10/10)\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle\n",
        "train_size = int(0.8 * len(df))\n",
        "val_size = int(0.1 * len(df))\n",
        "\n",
        "train_df = df[:train_size]\n",
        "val_df = df[train_size:train_size+val_size]\n",
        "test_df = df[train_size+val_size:]\n",
        "\n",
        "# Convert to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "product_reviews_dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "# Examine the dataset\n",
        "print(f\"Dataset structure: {product_reviews_dataset}\")\n",
        "print(f\"Number of examples in train split: {len(product_reviews_dataset['train'])}\")\n",
        "print(f\"Features: {product_reviews_dataset['train'].features}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Medical Reasoning with II-Medical-RL Dataset\n",
        "\n",
        "### 3.1 The Importance of Reasoning in Medical Contexts\n",
        "\n",
        "Medical decision-making requires robust reasoning capabilities. Clinicians must analyze symptoms, test results, and patient history to arrive at accurate diagnoses and treatment plans. This reasoning process is complex and often involves:\n",
        "\n",
        "- **Causal inference**: Understanding the relationship between symptoms and underlying conditions\n",
        "- **Evidence evaluation**: Weighing the significance of different clinical findings\n",
        "- **Hypothesis generation and testing**: Considering multiple possible diagnoses and ruling them out systematically\n",
        "- **Uncertainty management**: Making decisions with incomplete information\n",
        "\n",
        "Models that can generate high-quality medical reasoning have numerous potential applications:\n",
        "\n",
        "- **Clinical decision support**: Assisting healthcare providers with diagnostic reasoning\n",
        "- **Medical education**: Teaching students clinical reasoning patterns\n",
        "- **Research synthesis**: Summarizing and connecting findings across medical literature\n",
        "- **Patient communication**: Explaining medical rationales in accessible language\n",
        "\n",
        "Let's explore how to fine-tune a model for medical reasoning using the II-Medical-RL dataset, which is specifically designed for this purpose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Loading and Exploring the II-Medical-RL Dataset\n",
        "\n",
        "The II-Medical-RL dataset contains medical questions paired with expert reasoning and answers. It's structured in a \"question <sep> reasoning <sep> answer\" format, making it ideal for training models to generate reasoning given a medical question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the II-Medical-RL dataset from Hugging Face\n",
        "medical_rl_dataset = load_dataset(\"Intelligent-Internet/II-Medical-RL\")\n",
        "\n",
        "# Examine the dataset structure\n",
        "print(f\"Dataset structure: {medical_rl_dataset}\")\n",
        "print(f\"Available splits: {list(medical_rl_dataset.keys())}\")\n",
        "print(f\"Number of examples in train split: {len(medical_rl_dataset['train'])}\")\n",
        "print(f\"Features: {medical_rl_dataset['train'].features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Look at a few examples to understand the data format\n",
        "print(\"Sample example from II-Medical-RL dataset:\")\n",
        "sample_example = medical_rl_dataset['train'][0]\n",
        "print(f\"Question: {sample_example['question']}\")\n",
        "print(f\"\\nReasoning: {sample_example['reasoning']}\")\n",
        "print(f\"\\nAnswer: {sample_example['answer']}\")\n",
        "\n",
        "# Analyze the dataset statistics\n",
        "question_lengths = [len(example['question']) for example in medical_rl_dataset['train']]\n",
        "reasoning_lengths = [len(example['reasoning']) for example in medical_rl_dataset['train']]\n",
        "answer_lengths = [len(example['answer']) for example in medical_rl_dataset['train']]\n",
        "\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Average question length: {np.mean(question_lengths):.2f} characters\")\n",
        "print(f\"Average reasoning length: {np.mean(reasoning_lengths):.2f} characters\")\n",
        "print(f\"Average answer length: {np.mean(answer_lengths):.2f} characters\")\n",
        "print(f\"Max reasoning length: {np.max(reasoning_lengths)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the distribution of reasoning lengths\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(reasoning_lengths, bins=50, alpha=0.7, color='blue')\n",
        "plt.title('Distribution of Reasoning Lengths')\n",
        "plt.xlabel('Number of Characters')\n",
        "plt.ylabel('Frequency')\n",
        "plt.axvline(np.median(reasoning_lengths), color='red', linestyle='dashed', linewidth=1, label=f'Median: {np.median(reasoning_lengths):.0f}')\n",
        "plt.axvline(np.mean(reasoning_lengths), color='green', linestyle='dashed', linewidth=1, label=f'Mean: {np.mean(reasoning_lengths):.0f}')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Analyze label distribution\n",
        "if 'label' in medical_rl_dataset['train'].features:\n",
        "    label_counts = {}\n",
        "    for example in medical_rl_dataset['train']:\n",
        "        label = example['label']\n",
        "        if label in label_counts:\n",
        "            label_counts[label] += 1\n",
        "        else:\n",
        "            label_counts[label] = 1\n",
        "    \n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.bar(label_counts.keys(), label_counts.values())\n",
        "    plt.title('Distribution of Labels')\n",
        "    plt.xlabel('Label')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Preprocessing the II-Medical-RL Dataset\n",
        "\n",
        "Now that we've explored the dataset, let's preprocess it for fine-tuning a text generation model. We'll prepare the data for the task of generating reasoning given a medical question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create train/validation/test splits\n",
        "# The dataset already has a train split, let's create validation and test splits\n",
        "\n",
        "# Shuffle the dataset\n",
        "shuffled_dataset = medical_rl_dataset['train'].shuffle(seed=42)\n",
        "\n",
        "# Split into train (80%), validation (10%), and test (10%)\n",
        "train_size = int(0.8 * len(shuffled_dataset))\n",
        "val_size = int(0.1 * len(shuffled_dataset))\n",
        "\n",
        "train_dataset = shuffled_dataset.select(range(train_size))\n",
        "val_dataset = shuffled_dataset.select(range(train_size, train_size + val_size))\n",
        "test_dataset = shuffled_dataset.select(range(train_size + val_size, len(shuffled_dataset)))\n",
        "\n",
        "# Create a DatasetDict\n",
        "medical_dataset = DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'validation': val_dataset,\n",
        "    'test': test_dataset\n",
        "})\n",
        "\n",
        "print(f\"Train set size: {len(medical_dataset['train'])}\")\n",
        "print(f\"Validation set size: {len(medical_dataset['validation'])}\")\n",
        "print(f\"Test set size: {len(medical_dataset['test'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Format the data for a text generation task\n",
        "# We'll use the question as input and the reasoning as the target output\n",
        "\n",
        "def format_for_generation(example):\n",
        "    \"\"\"Format the example for text generation (question → reasoning)\"\"\"\n",
        "    return {\n",
        "        'input_text': f\"Question: {example['question']}\\nGenerate medical reasoning:\",\n",
        "        'target_text': example['reasoning']\n",
        "    }\n",
        "\n",
        "# Apply the formatting function\n",
        "formatted_medical_dataset = medical_dataset.map(format_for_generation)\n",
        "\n",
        "# Show a sample\n",
        "print(\"Formatted example:\")\n",
        "sample = formatted_medical_dataset['train'][0]\n",
        "print(f\"Input:\\n{sample['input_text']}\")\n",
        "print(f\"\\nTarget:\\n{sample['target_text']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Preparing for Fine-tuning a Text Generation Model\n",
        "\n",
        "Now we'll prepare the data for fine-tuning a GPT-2 model for medical reasoning generation. We'll use the Hugging Face Transformers library for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a GPT-2 tokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"gpt2\"  # We'll use the small GPT-2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# GPT-2 doesn't have a pad token by default, so we'll set it to the EOS token\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize the data for causal language modeling\n",
        "# For GPT-2, we concatenate the input and target with a separator\n",
        "\n",
        "def tokenize_for_generation(examples):\n",
        "    \"\"\"Tokenize examples for causal language modeling\"\"\"\n",
        "    # Concatenate input and target\n",
        "    texts = [inp + \"\\n\" + tgt for inp, tgt in zip(examples['input_text'], examples['target_text'])]\n",
        "    \n",
        "    # Tokenize\n",
        "    tokenized = tokenizer(texts, padding='max_length', truncation=True, max_length=1024, return_tensors=\"pt\")\n",
        "    \n",
        "    # Create labels (same as input_ids for causal LM)\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    \n",
        "    return tokenized\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_medical_dataset = formatted_medical_dataset.map(\n",
        "    tokenize_for_generation,\n",
        "    batched=True,\n",
        "    remove_columns=['question', 'reasoning', 'answer', 'options', 'problem', 'label', 'dataset_name', 'id_in_dataset', 'input_text', 'target_text']\n",
        ")\n",
        "\n",
        "# Set the format for PyTorch\n",
        "tokenized_medical_dataset = tokenized_medical_dataset.with_format(\"torch\")\n",
        "\n",
        "print(tokenized_medical_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Fine-tuning GPT-2 for Medical Reasoning Generation\n",
        "\n",
        "Now we'll fine-tune a GPT-2 model to generate medical reasoning given a question. This task is particularly valuable as it helps the model develop structured reasoning capabilities in a medical context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the pre-trained GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/medical_reasoning_model\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,  # Adjust based on your GPU memory\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\",\n",
        "    gradient_accumulation_steps=4,  # To effectively increase batch size\n",
        "    fp16=True,  # Use mixed precision training if available\n",
        "    logging_steps=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a custom evaluation metric for text generation\n",
        "import evaluate\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute ROUGE metrics for text generation evaluation\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    \n",
        "    # Decode the predictions and labels\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    \n",
        "    # Replace -100 with the pad_token_id\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the generated reasoning part\n",
        "    processed_preds = []\n",
        "    processed_labels = []\n",
        "    \n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        # Extract reasoning from prediction (after the prompt)\n",
        "        if \"Generate medical reasoning:\" in pred:\n",
        "            pred_reasoning = pred.split(\"Generate medical reasoning:\", 1)[1].strip()\n",
        "        else:\n",
        "            pred_reasoning = pred\n",
        "        \n",
        "        # Extract reasoning from label (after the prompt)\n",
        "        if \"Generate medical reasoning:\" in label:\n",
        "            label_reasoning = label.split(\"Generate medical reasoning:\", 1)[1].strip()\n",
        "        else:\n",
        "            label_reasoning = label\n",
        "        \n",
        "        processed_preds.append(pred_reasoning)\n",
        "        processed_labels.append(label_reasoning)\n",
        "    \n",
        "    # Compute ROUGE scores\n",
        "    result = rouge.compute(predictions=processed_preds, references=processed_labels, use_stemmer=True)\n",
        "    \n",
        "    return {\n",
        "        'rouge1': result['rouge1'],\n",
        "        'rouge2': result['rouge2'],\n",
        "        'rougeL': result['rougeL']\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer for text generation\n",
        "from transformers import Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We're doing causal LM, not masked LM\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_medical_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_medical_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    # compute_metrics=compute_metrics,  # Uncomment if you want to use custom metrics\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "# Note: This will take some time depending on your hardware\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./final_model/medical_reasoning_model\")\n",
        "tokenizer.save_pretrained(\"./final_model/medical_reasoning_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.6 Evaluating the Medical Reasoning Model\n",
        "\n",
        "Now let's evaluate our fine-tuned model on the test set and see how well it can generate medical reasoning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the fine-tuned model\n",
        "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"./final_model/medical_reasoning_model\")\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"./final_model/medical_reasoning_model\")\n",
        "\n",
        "# Make sure the pad token is set\n",
        "if fine_tuned_tokenizer.pad_token is None:\n",
        "    fine_tuned_tokenizer.pad_token = fine_tuned_tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to generate reasoning for a given question\n",
        "def generate_reasoning(question, max_length=500):\n",
        "    \"\"\"Generate medical reasoning for a given question\"\"\"\n",
        "    # Prepare the input prompt\n",
        "    prompt = f\"Question: {question}\\nGenerate medical reasoning:\"\n",
        "    \n",
        "    # Tokenize the input\n",
        "    inputs = fine_tuned_tokenizer(prompt, return_tensors=\"pt\").to(fine_tuned_model.device)\n",
        "    \n",
        "    # Generate text\n",
        "    outputs = fine_tuned_model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        pad_token_id=fine_tuned_tokenizer.pad_token_id\n",
        "    )\n",
        "    \n",
        "    # Decode the generated text\n",
        "    generated_text = fine_tuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Extract only the reasoning part (after the prompt)\n",
        "    if \"Generate medical reasoning:\" in generated_text:\n",
        "        reasoning = generated_text.split(\"Generate medical reasoning:\", 1)[1].strip()\n",
        "    else:\n",
        "        reasoning = generated_text.replace(prompt, \"\").strip()\n",
        "    \n",
        "    return reasoning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the model on a few examples from the test set\n",
        "test_examples = medical_dataset['test'].select(range(3))\n",
        "\n",
        "for i, example in enumerate(test_examples):\n",
        "    question = example['question']\n",
        "    true_reasoning = example['reasoning']\n",
        "    true_answer = example['answer']\n",
        "    \n",
        "    # Generate reasoning\n",
        "    generated_reasoning = generate_reasoning(question)\n",
        "    \n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"\\nGenerated Reasoning:\\n{generated_reasoning}\")\n",
        "    print(f\"\\nTrue Reasoning:\\n{true_reasoning}\")\n",
        "    print(f\"\\nTrue Answer: {true_answer}\")\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on a larger subset of the test set\n",
        "from tqdm.auto import tqdm\n",
        "import random\n",
        "\n",
        "# Select a random subset of test examples for evaluation\n",
        "num_eval_examples = 50\n",
        "eval_indices = random.sample(range(len(medical_dataset['test'])), num_eval_examples)\n",
        "eval_examples = medical_dataset['test'].select(eval_indices)\n",
        "\n",
        "# Prepare for ROUGE evaluation\n",
        "generated_reasonings = []\n",
        "true_reasonings = []\n",
        "\n",
        "# Generate reasonings for evaluation examples\n",
        "for example in tqdm(eval_examples, desc=\"Generating reasonings\"):\n",
        "    question = example['question']\n",
        "    true_reasoning = example['reasoning']\n",
        "    \n",
        "    # Generate reasoning\n",
        "    generated_reasoning = generate_reasoning(question)\n",
        "    \n",
        "    generated_reasonings.append(generated_reasoning)\n",
        "    true_reasonings.append(true_reasoning)\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_results = rouge.compute(\n",
        "    predictions=generated_reasonings,\n",
        "    references=true_reasonings,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "print(f\"ROUGE-1: {rouge_results['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_results['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_results['rougeL']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.7 Using the Fine-tuned Model for Medical Reasoning\n",
        "\n",
        "Now that we have a fine-tuned model, let's use it to generate reasoning for some new medical questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define some new medical questions\n",
        "new_questions = [\n",
        "    \"Does regular exercise reduce the risk of cardiovascular disease?\",\n",
        "    \"Can vitamin D deficiency contribute to depression?\",\n",
        "    \"Is there a connection between gut microbiome and autoimmune disorders?\"\n",
        "]\n",
        "\n",
        "# Generate reasoning for each question\n",
        "for question in new_questions:\n",
        "    reasoning = generate_reasoning(question)\n",
        "    \n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"\\nGenerated Reasoning:\\n{reasoning}\")\n",
        "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.8 Conclusion: Medical Reasoning Fine-tuning\n",
        "\n",
        "In this section, we've demonstrated how to fine-tune a language model for medical reasoning using the II-Medical-RL dataset. This approach has several key advantages:\n",
        "\n",
        "1. **Structured Reasoning**: The model learns to generate step-by-step reasoning rather than just answers, making its outputs more transparent and trustworthy.\n",
        "\n",
        "2. **Domain Adaptation**: By fine-tuning on medical data, the model adapts its knowledge to the medical domain, learning medical terminology and concepts.\n",
        "\n",
        "3. **Explainability**: The generated reasoning provides insight into how the model arrived at its conclusions, which is crucial in high-stakes domains like healthcare.\n",
        "\n",
        "4. **Educational Value**: The model can be used to teach clinical reasoning patterns to medical students or to help patients understand medical concepts.\n",
        "\n",
        "While this fine-tuned model shows promise, it's important to note that it should not be used for actual medical decision-making without proper validation and oversight by healthcare professionals. The goal is to assist, not replace, human medical expertise.\n",
        "\n",
        "Next, we'll continue with our exploration of fine-tuning for other tasks and domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Selecting a Base Model\n",
        "\n",
        "Choosing the right pre-trained model is crucial for successful fine-tuning. The selection depends on your task, data, and computational constraints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Criteria for Selecting Pre-trained Models\n",
        "\n",
        "Consider these factors when selecting a base model:\n",
        "\n",
        "1. **Task Compatibility**: Choose a model pre-trained on a task similar to yours\n",
        "2. **Domain Relevance**: Consider models trained on data from your domain\n",
        "3. **Model Size**: Larger models generally perform better but require more resources\n",
        "4. **Computational Requirements**: Consider your hardware constraints\n",
        "5. **Community Support**: Popular models have better documentation and resources\n",
        "\n",
        "### 4.2 Model Architectures for Different Tasks\n",
        "\n",
        "| Task | Common Architectures | Example Models |\n",
        "|------|---------------------|----------------|\n",
        "| Text Classification | BERT, RoBERTa, DistilBERT | distilbert-base-uncased, roberta-base |\n",
        "| Named Entity Recognition | BERT, RoBERTa, LUKE | bert-base-NER, roberta-large |\n",
        "| Question Answering | BERT, RoBERTa, T5 | deepset/roberta-base-squad2, t5-base |\n",
        "| Text Generation | GPT-2, T5, BART | gpt2, t5-base, facebook/bart-large |\n",
        "| Image Classification | ResNet, ViT, Swin | google/vit-base-patch16-224, microsoft/resnet-50 |\n",
        "| Object Detection | DETR, Faster R-CNN | facebook/detr-resnet-50, faster-rcnn |\n",
        "| Image Segmentation | SegFormer, DeepLabV3 | nvidia/segformer-b0-finetuned-ade-512-512 |\n",
        "| Speech Recognition | Wav2Vec2, HuBERT | facebook/wav2vec2-base-960h |\n",
        "\n",
        "### 4.3 Browsing Models on the Hugging Face Hub\n",
        "\n",
        "The Hugging Face Hub hosts thousands of pre-trained models. You can browse them at https://huggingface.co/models.\n",
        "\n",
        "Let's see how to programmatically explore models for our tasks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the HfAPI to search for models\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# Search for text classification models\n",
        "text_models = api.list_models(filter=\"text-classification\", sort=\"downloads\", direction=-1, limit=5)\n",
        "\n",
        "print(\"Top 5 Text Classification Models:\")\n",
        "for model in text_models:\n",
        "    print(f\"- {model.id} (Downloads: {model.downloads})\")\n",
        "\n",
        "# Search for image classification models\n",
        "image_models = api.list_models(filter=\"image-classification\", sort=\"downloads\", direction=-1, limit=5)\n",
        "\n",
        "print(\"\\nTop 5 Image Classification Models:\")\n",
        "for model in image_models:\n",
        "    print(f\"- {model.id} (Downloads: {model.downloads})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For our text classification task, we'll use `distilbert-base-uncased`, a smaller, faster version of BERT that still performs well. For image classification, we'll use `google/vit-base-patch16-224`, a Vision Transformer model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training and Fine-tuning\n",
        "\n",
        "Now that we have our data prepared and have selected our base models, let's implement the fine-tuning process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Text Classification Fine-tuning\n",
        "\n",
        "We'll fine-tune DistilBERT on our product reviews dataset for sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the pre-trained model for sequence classification\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "num_labels = 2  # binary classification (positive/negative)\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name, \n",
        "    num_labels=num_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define metrics for evaluation\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute metrics for evaluation\"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    \n",
        "    # Calculate multiple metrics\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    precision = precision_score(labels, predictions, average=\"weighted\")\n",
        "    recall = recall_score(labels, predictions, average=\"weighted\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1\": f1,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/sentiment_model\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,  # Set to True if you want to upload to Hugging Face Hub\n",
        "    report_to=\"none\"  # Disable reporting to avoid dependencies\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on the test set\n",
        "test_results = trainer.evaluate(tokenized_dataset[\"test\"])\n",
        "print(f\"Test results: {test_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the model\n",
        "trainer.save_model(\"./final_model/sentiment_model\")\n",
        "tokenizer.save_pretrained(\"./final_model/sentiment_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Image Classification Fine-tuning\n",
        "\n",
        "Now let's fine-tune a Vision Transformer (ViT) model on the CIFAR-10 dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the pre-trained model for image classification\n",
        "model_name = \"google/vit-base-patch16-224\"\n",
        "num_labels = 10  # CIFAR-10 has 10 classes\n",
        "\n",
        "# Get label names for CIFAR-10\n",
        "label_names = cifar10_dataset[\"train\"].features[\"label\"].names\n",
        "print(f\"CIFAR-10 classes: {label_names}\")\n",
        "\n",
        "# Load the model with the correct number of labels\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=num_labels,\n",
        "    ignore_mismatched_sizes=True  # Important when changing the number of labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure training arguments for image classification\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/cifar10_model\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the Trainer for image classification\n",
        "image_trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=prepared_cifar[\"train\"],\n",
        "    eval_dataset=prepared_cifar[\"validation\"],\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the image classification model\n",
        "image_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the image classification model\n",
        "image_test_results = image_trainer.evaluate()\n",
        "print(f\"Image classification results: {image_test_results}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the image classification model\n",
        "image_trainer.save_model(\"./final_model/cifar10_model\")\n",
        "image_processor.save_pretrained(\"./final_model/cifar10_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation\n",
        "\n",
        "Now that we've trained our models, let's evaluate them more thoroughly and perform error analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Text Classification Evaluation\n",
        "\n",
        "Let's evaluate our sentiment analysis model on the test set and analyze its errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions on the test set\n",
        "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# Get the original texts and true labels\n",
        "test_df = df[train_size+val_size:].reset_index(drop=True)\n",
        "test_texts = test_df[\"review_text\"].tolist()\n",
        "true_labels = [1 if s == \"positive\" else 0 for s in test_df[\"sentiment\"].tolist()]\n",
        "\n",
        "# Create a DataFrame with predictions\n",
        "results_df = pd.DataFrame({\n",
        "    \"text\": test_texts,\n",
        "    \"true_label\": true_labels,\n",
        "    \"predicted_label\": preds,\n",
        "    \"correct\": np.equal(true_labels, preds)\n",
        "})\n",
        "\n",
        "# Calculate overall accuracy\n",
        "accuracy = accuracy_score(true_labels, preds)\n",
        "print(f\"Overall accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Look at the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "cm = confusion_matrix(true_labels, preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Negative\", \"Positive\"])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Error analysis: look at misclassified examples\n",
        "errors_df = results_df[~results_df[\"correct\"]].copy()\n",
        "print(f\"Number of errors: {len(errors_df)} out of {len(results_df)} examples\")\n",
        "\n",
        "# Add human-readable labels\n",
        "errors_df[\"true_sentiment\"] = errors_df[\"true_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
        "errors_df[\"predicted_sentiment\"] = errors_df[\"predicted_label\"].map({0: \"Negative\", 1: \"Positive\"})\n",
        "\n",
        "# Display some misclassified examples\n",
        "print(\"\\nSample of misclassified examples:\")\n",
        "sample_errors = errors_df.sample(min(5, len(errors_df)))\n",
        "for i, row in sample_errors.iterrows():\n",
        "    print(f\"\\nText: {row['text']}\")\n",
        "    print(f\"True sentiment: {row['true_sentiment']}\")\n",
        "    print(f\"Predicted sentiment: {row['predicted_sentiment']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Image Classification Evaluation\n",
        "\n",
        "Now let's evaluate our image classification model and analyze its errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions on the validation set\n",
        "image_predictions = image_trainer.predict(prepared_cifar[\"validation\"])\n",
        "image_preds = np.argmax(image_predictions.predictions, axis=-1)\n",
        "true_image_labels = prepared_cifar[\"validation\"][\"labels\"]\n",
        "\n",
        "# Calculate accuracy\n",
        "image_accuracy = accuracy_score(true_image_labels, image_preds)\n",
        "print(f\"Image classification accuracy: {image_accuracy:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "image_cm = confusion_matrix(true_image_labels, image_preds)\n",
        "image_disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=image_cm, \n",
        "    display_labels=label_names\n",
        ")\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "image_disp.plot(ax=ax, cmap=\"Blues\")\n",
        "plt.title(\"CIFAR-10 Confusion Matrix\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize some misclassified images\n",
        "misclassified_indices = np.where(image_preds != true_image_labels)[0]\n",
        "print(f\"Number of misclassified images: {len(misclassified_indices)} out of {len(true_image_labels)}\")\n",
        "\n",
        "# Display some misclassified examples\n",
        "if len(misclassified_indices) > 0:\n",
        "    num_examples = min(10, len(misclassified_indices))\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, idx in enumerate(misclassified_indices[:num_examples]):\n",
        "        # Get the original image\n",
        "        original_idx = cifar_small[\"validation\"][idx][\"id\"]\n",
        "        img = cifar10_dataset[\"test\"][original_idx][\"img\"]\n",
        "        \n",
        "        true_label = true_image_labels[idx]\n",
        "        pred_label = image_preds[idx]\n",
        "        \n",
        "        axes[i].imshow(img)\n",
        "        axes[i].set_title(f\"True: {label_names[true_label]}\\nPred: {label_names[pred_label]}\")\n",
        "        axes[i].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Performance Comparison\n",
        "\n",
        "Let's compare our fine-tuned models to some baselines to understand the benefits of fine-tuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For text classification, compare to a simple baseline (TF-IDF + LogisticRegression)\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Prepare data\n",
        "train_texts = train_df[\"review_text\"].tolist()\n",
        "train_labels = [1 if s == \"positive\" else 0 for s in train_df[\"sentiment\"].tolist()]\n",
        "\n",
        "test_texts = test_df[\"review_text\"].tolist()\n",
        "test_labels = [1 if s == \"positive\" else 0 for s in test_df[\"sentiment\"].tolist()]\n",
        "\n",
        "# Create a baseline model\n",
        "baseline_model = Pipeline([\n",
        "    (\"vectorizer\", TfidfVectorizer(max_features=5000)),\n",
        "    (\"classifier\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "# Train and evaluate\n",
        "baseline_model.fit(train_texts, train_labels)\n",
        "baseline_preds = baseline_model.predict(test_texts)\n",
        "baseline_accuracy = accuracy_score(test_labels, baseline_preds)\n",
        "\n",
        "print(f\"Baseline (TF-IDF + LogReg) accuracy: {baseline_accuracy:.4f}\")\n",
        "print(f\"Fine-tuned DistilBERT accuracy: {accuracy:.4f}\")\n",
        "print(f\"Improvement: {(accuracy - baseline_accuracy) * 100:.2f} percentage points\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Deployment\n",
        "\n",
        "Once you've fine-tuned and evaluated your model, the next step is to deploy it for inference. There are several options for deploying Hugging Face models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Local Inference\n",
        "\n",
        "The simplest deployment option is to load your saved model for local inference:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the saved sentiment analysis model\n",
        "from transformers import pipeline\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"./final_model/sentiment_model\",\n",
        "    tokenizer=\"./final_model/sentiment_model\"\n",
        ")\n",
        "\n",
        "# Test with some examples\n",
        "test_examples = [\n",
        "    \"This product is amazing! I love it.\",\n",
        "    \"Terrible quality, broke after one use.\",\n",
        "    \"Average product, nothing special but works fine.\"\n",
        "]\n",
        "\n",
        "# Run inference\n",
        "results = sentiment_analyzer(test_examples)\n",
        "\n",
        "# Display results\n",
        "for text, result in zip(test_examples, results):\n",
        "    label = \"Positive\" if result[\"label\"] == \"LABEL_1\" else \"Negative\"\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Sentiment: {label} (confidence: {result['score']:.4f})\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the saved image classification model\n",
        "image_classifier = pipeline(\n",
        "    \"image-classification\",\n",
        "    model=\"./final_model/cifar10_model\",\n",
        "    feature_extractor=\"./final_model/cifar10_model\"\n",
        ")\n",
        "\n",
        "# Test with some examples from CIFAR-10\n",
        "test_images = [cifar10_dataset[\"test\"][i][\"img\"] for i in range(5)]\n",
        "\n",
        "# Run inference\n",
        "image_results = image_classifier(test_images)\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
        "for i, (img, result) in enumerate(zip(test_images, image_results)):\n",
        "    axes[i].imshow(img)\n",
        "    top_label = result[0][\"label\"]\n",
        "    confidence = result[0][\"score\"]\n",
        "    axes[i].set_title(f\"{top_label}\\n{confidence:.2f}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Creating a Simple API with FastAPI\n",
        "\n",
        "For production use, you might want to create an API. Here's a simple example using FastAPI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%writefile sentiment_api.py\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "import uvicorn\n",
        "\n",
        "# Define the request model\n",
        "class SentimentRequest(BaseModel):\n",
        "    text: str\n",
        "\n",
        "# Initialize FastAPI\n",
        "app = FastAPI(title=\"Sentiment Analysis API\")\n",
        "\n",
        "# Load the model at startup\n",
        "@app.on_event(\"startup\")\n",
        "def load_model():\n",
        "    global sentiment_analyzer\n",
        "    sentiment_analyzer = pipeline(\n",
        "        \"text-classification\",\n",
        "        model=\"./final_model/sentiment_model\",\n",
        "        tokenizer=\"./final_model/sentiment_model\"\n",
        "    )\n",
        "\n",
        "# Define the prediction endpoint\n",
        "@app.post(\"/predict\")\n",
        "def predict_sentiment(request: SentimentRequest):\n",
        "    try:\n",
        "        # Run inference\n",
        "        result = sentiment_analyzer(request.text)[0]\n",
        "        \n",
        "        # Convert to a more user-friendly format\n",
        "        sentiment = \"positive\" if result[\"label\"] == \"LABEL_1\" else \"negative\"\n",
        "        confidence = result[\"score\"]\n",
        "        \n",
        "        return {\n",
        "            \"text\": request.text,\n",
        "            \"sentiment\": sentiment,\n",
        "            \"confidence\": confidence\n",
        "        }\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Health check endpoint\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    return {\"status\": \"healthy\"}\n",
        "\n",
        "# Run the API server when the script is executed directly\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(\"sentiment_api:app\", host=\"0.0.0.0\", port=8000, reload=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run this API, you would execute:\n",
        "\n",
        "```bash\n",
        "python sentiment_api.py\n",
        "```\n",
        "\n",
        "Then you can make requests to `http://localhost:8000/predict` with JSON data like `{\"text\": \"I love this product!\"}`. The API also provides automatic documentation at `http://localhost:8000/docs`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.3 Model Optimization\n",
        "\n",
        "Before deploying to production, you might want to optimize your model for inference speed and memory usage. Common techniques include:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Quantization\n",
        "\n",
        "Quantization reduces the precision of the model weights, typically from 32-bit floating point to 8-bit integers, significantly reducing model size and improving inference speed with minimal impact on accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of dynamic quantization with PyTorch\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model_path = \"./final_model/sentiment_model\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "\n",
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, \n",
        "    {torch.nn.Linear}, \n",
        "    dtype=torch.qint8\n",
        ")\n",
        "\n",
        "# Save the quantized model\n",
        "quantized_model_path = \"./final_model/sentiment_model_quantized\"\n",
        "quantized_model.save_pretrained(quantized_model_path)\n",
        "\n",
        "# Compare model sizes\n",
        "import os\n",
        "\n",
        "def get_model_size(path):\n",
        "    total_size = 0\n",
        "    for dirpath, dirnames, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "original_size = get_model_size(model_path)\n",
        "quantized_size = get_model_size(quantized_model_path)\n",
        "\n",
        "print(f\"Original