{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23db2c4f",
   "metadata": {},
   "source": [
    "# Finetune your own model\n",
    "\n",
    "\n",
    "This notebook aims to walk you through all the steps needed to finetune your own transformer using the Huggingface `transformers` ecosystem. \n",
    "\n",
    "### Motivation\n",
    "Why finetune a transformer? Finetuning is useful whenever you wish to have a specialized model for a specific task. Training competitively-sized transformers from scratch these days requires an immense amount of resources, both in data and in compute. On the other hand, prompt engineering is very fast, but has a limited scope and can't really teach the model entirely new concepts. Finetuning attempts to strike a middle ground for tasks that still, for example, require an understanding of language, but require more specialization than prompt engineering can provide.\n",
    "\n",
    "### Setup and Considerations\n",
    "Before finetuning a transformer, you have to decide what to finetune it *on*. What you choose depends on what kind of task you want it to accomplish. In this case, I will be finetuning DistilGPT-2 on Shakespeare's sonnets, but you can pick anything you would like. It is also possible to finetune, say, image generation models on a set of images, but the code here would need to be modified slightly. \n",
    "\n",
    "One consideration that is a fundamental issue in finetuning is the tradeoff between catastrophic forgetting and specialization. That is, by finetuning it on new data, the model may forget parts of what it learned when it was originally trained. Different strategies can be used to mitigate this and find the right tradeoff. \n",
    "\n",
    "---\n",
    "\n",
    "With that out of the way, let's get to some code!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071805da",
   "metadata": {},
   "source": [
    "Import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fad725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Import and install necessary libraries\n",
    "import sys\n",
    "if sys.prefix != sys.base_prefix or \"google.colab\" in sys.modules:\n",
    "    pass\n",
    "else:\n",
    "    print(\"Not in a virtual environment. Please create one and activate it before running this script.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "!pip install 'accelerate>=0.26.0' 'transformers[torch]' datasets torch \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8a8b0",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "The first step to training any model is to prepare the data. Often, when working with custom datasets, this is the step that takes the most work! In this case, we are using a pre-built dataset, so Huggingface lets us load this in automatically. Here, I've provided an example of how you might further process individual data samples and use `.map` to apply it. A slightly different, more declarative strategy than the Pytorch paradigm of creating a custom Dataset class with lots of boilerplate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97775cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_and_preprocess_data():\n",
    "    # Load the dataset from Hugging Face\n",
    "    sonnets_dataset = load_dataset(\"kkawamu1/shakespeares_sonnets\")\n",
    "    \n",
    "    # Create train/validation split (90/10)\n",
    "    # If you have an extremely limited dataset, you could employ other methods such as k-fold cross validation. \n",
    "    shuffled_dataset = sonnets_dataset['train'].shuffle(seed=42)\n",
    "    train_size = int(0.9 * len(shuffled_dataset))\n",
    "    \n",
    "    train_dataset = shuffled_dataset.select(range(train_size))\n",
    "    val_dataset = shuffled_dataset.select(range(train_size, len(shuffled_dataset)))\n",
    "    \n",
    "    print(f\"Train set size: {len(train_dataset)}, Validation set size: {len(val_dataset)}\")\n",
    "    \n",
    "    # Example of preprocessing: lowercasing and stripping whitespace\n",
    "    def clean_text(example):\n",
    "        # Crude insertion of <|eos|>. In reality, the token should probably be explicitly added to the tokenizer. You don't have to worry about this for now.\n",
    "        example[\"text\"] = example[\"text\"].lower().strip() + \"<|eos|>\"\n",
    "\n",
    "        return example\n",
    "    \n",
    "    train_dataset = train_dataset.map(clean_text)\n",
    "    val_dataset = val_dataset.map(clean_text)\n",
    "    \n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521f966",
   "metadata": {},
   "source": [
    "### Further Data Preparation\n",
    "\n",
    "Transformer models, internally, receive and generate tokens. Thus, we need to tokenize our dataset. Here, a premade tokenizer comes with `distilgpt2`, so we can use that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4220f934",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_for_training(train_dataset, val_dataset):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "    \n",
    "    # Has to be done for distilgpt2 in order to ensure there exists a pad token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Tokenize the data (just calls tokenizer.encode on each text)\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    \n",
    "    tokenized_train = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"id\"])\n",
    "    tokenized_val = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"id\"])\n",
    "    \n",
    "    # Set the format for PyTorch\n",
    "    tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    tokenized_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    \n",
    "    return tokenizer, tokenized_train, tokenized_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5427c3",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Now for the most exciting part, training the model! Fortunately, HF does a lot of the work for us, so this is mostly about specifying hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea97cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(tokenizer, train_dataset, val_dataset):\n",
    "\n",
    "    # Loads the pre-trained DistilGPT-2 model\n",
    "    # Other great pretrained models exist, depending on how much compute you have\n",
    "    # A list to get you started: https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src/transformers/models/auto/modeling_auto.py#L559\n",
    "    # But any model on the Hub works\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\") \n",
    "    \n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=5, # Train for 5 epochs\n",
    "        per_device_train_batch_size=4, # Batch size of 4. If you have more GPU memory, you can increase this\n",
    "        per_device_eval_batch_size=4,\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        warmup_steps=100,\n",
    "        eval_strategy=\"steps\",\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        learning_rate=5e-5, # Typical starting point for fine-tuning\n",
    "        save_total_limit=2,  # Keep only the last 2 models\n",
    "        weight_decay=0.01,\n",
    "        fp16=bool(torch.cuda.is_available()),\n",
    "    )\n",
    "    \n",
    "    # Data collator. Think of this like a pytorch DataLoader, but from Hugging Face.\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False  # We're doing causal language modeling, not masked\n",
    "    )\n",
    "    \n",
    "    # Initialize the Trainer. Pass in model, arguments, collator, and datasets.\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    # Huggingface does the rest!\n",
    "    trainer.train()\n",
    "    # If resuming from a checkpoint\n",
    "    # trainer.train(resume_from_checkpoint=True)\n",
    "    \n",
    "    return model, trainer\n",
    "\n",
    "# Utility function demonstrating `save_pretrained`\n",
    "MODEL_PATH = \"./shakespeare_sonnets_model\"\n",
    "def save_model(model, tokenizer):\n",
    "    output_dir = MODEL_PATH\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ff4929",
   "metadata": {},
   "source": [
    "### Generation\n",
    "\n",
    "After training, we can now use our model to generate samples. Here, you need to tokenize the input, run it through the model and, decode the output back into text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1138e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(prompt=\"Shall I compare thee \", max_length=250):\n",
    "    try:\n",
    "        # Load model and tokenizer from specified path\n",
    "        model_dir = MODEL_PATH\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        # Set pad token, again\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "            \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\") #Tokenize input prompt\n",
    "        \n",
    "        outputs = model.generate( # Generate output. Huggingface handles the internals.\n",
    "            inputs.input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.9,\n",
    "            top_p=0.92,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True) # Decode the generated tokens\n",
    "\n",
    "        generated_text = generated_text[len(prompt):].split(\"<|eos|>\")[0].strip()\n",
    "        \n",
    "        return generated_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating sonnet: {e}\")\n",
    "        return \"Could not generate sonnet. Make sure the model has been trained and saved.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038573df",
   "metadata": {},
   "source": [
    "### Running\n",
    "\n",
    "Run the following cell to run the code and see the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f684b8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    train_dataset, val_dataset = load_and_preprocess_data()\n",
    "\n",
    "    tokenizer, tokenized_train, tokenized_val = prepare_for_training(train_dataset, val_dataset)\n",
    "\n",
    "    model, trainer = train_model(tokenizer, tokenized_train, tokenized_val)\n",
    "\n",
    "    model_dir = save_model(model, tokenizer)\n",
    "\n",
    "    print(\"\\nGenerating a sample sonnet...\")\n",
    "    sample_sonnet = generate(\"Forsooth for I shall not\")\n",
    "    print(sample_sonnet)\n",
    "    # Output may vary\n",
    "    \"\"\"\n",
    "        depart\n",
    "        with a sweet night,\n",
    "        for the beauty of my mind will see;\n",
    "        for this summer is no year in the summer\n",
    "        which lies on the end.\n",
    "        but i will stay with those that have my ear,\n",
    "        and my heart to my heart where love is,\n",
    "        when that night shall show\n",
    "        as if i do not renew my life:\n",
    "        as with a summer which brings for myself,\n",
    "        it knows not where i dwell, and keeps me alive.\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
